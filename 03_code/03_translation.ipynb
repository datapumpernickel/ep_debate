{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"translate_ep_speeches.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1_IH0JaUFfoec2PQqA12_J1x9P58YXWF3\n",
    "\"\"\"\n",
    "\n",
    "#!pip install -U pip transformers\n",
    "#!pip install sentencepiece\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "full_text = pd.read_csv(\"../04_clean_data/language_detection.csv\")\n",
    "data = pd.read_csv(\"../04_clean_data/missing_speeches_parlee_sents.csv\")\n",
    "\n",
    "\n",
    "data.dropna(subset=['sentence'], inplace=True)\n",
    "data.reset_index(inplace = True)\n",
    "\n",
    "filtered_data = pd.merge(data, full_text, on='text_id', how='left')\n",
    "\n",
    "filtered_data = filtered_data[filtered_data['language'] != 'eng_Latn']\n",
    "\n",
    "# Keep only specific columns\n",
    "filtered_data = filtered_data[['text_id', 'session_id', 'id_speaker', 'Sentence_id', 'sentence','language']]\n",
    "\n",
    "filtered_data.reset_index(inplace = True)\n",
    "filtered_data.drop_duplicates(subset='sentence', inplace = True)\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data['language'].unique()\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "checkpoint = 'facebook/nllb-200-distilled-600M'\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 11:17:24,653\tINFO worker.py:1642 -- Started a local Ray instance.\n",
      "Translating by Language:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slk_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating by Language:   5%|▌         | 1/20 [01:52<35:39, 112.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hun_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating by Language:  10%|█         | 2/20 [04:29<41:31, 138.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ces_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch starting at index 7000, Start time: 2023-11-08 11:24:13.522547\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import ray\n",
    "from transformers import pipeline\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Ray cluster once, outside of loops\n",
    "ray.init(num_cpus=20, ignore_reinit_error=True)\n",
    "\n",
    "# @ray.remote decorator enables to use this \n",
    "# function in distributed setting\n",
    "@ray.remote\n",
    "def predict(pipeline, text_data):\n",
    "    return pipeline(text_data)\n",
    "\n",
    "# Open the JSONL file to store the translations\n",
    "with open('translated_sentences.jsonl', 'a+') as f:\n",
    "    unique_languages = filtered_data['language'].unique()[17:]\n",
    "    for current_language in tqdm(unique_languages, desc=\"Translating by Language\"):\n",
    "        print(current_language)\n",
    "        # Filter the DataFrame to only include rows with the current language\n",
    "        language_specific_data = filtered_data[filtered_data['language'] == current_language]\n",
    "\n",
    "        # Initialize the translation pipeline once per language\n",
    "        translation_pipeline = pipeline('translation',\n",
    "                                        model=model,\n",
    "                                        tokenizer=tokenizer,\n",
    "                                        src_lang=current_language,\n",
    "                                        tgt_lang='eng_Latn',\n",
    "                                        max_length=500)\n",
    "        pipe_id = ray.put(translation_pipeline)\n",
    "\n",
    "        batch_size = 1000\n",
    "        # Process sentences in batches\n",
    "        for i in tqdm(range(0, len(language_specific_data), batch_size), desc=f\"Translating Sentences in {current_language}\", leave=False):\n",
    "\n",
    "            batch_data = language_specific_data.iloc[i:i+batch_size]\n",
    "            batch_sentences = batch_data['sentence'].tolist()\n",
    "\n",
    "            # Check existing IDs and filter new batch data\n",
    "            f.seek(0)\n",
    "            existing_ids = [(json.loads(line).get('text_id'), json.loads(line).get('Sentence_id')) for line in f]\n",
    "            new_batch_data = [(row['text_id'], row['Sentence_id'], row['sentence']) for index, row in batch_data.iterrows() if (row['text_id'], row['Sentence_id']) not in existing_ids]\n",
    "            \n",
    "            if not new_batch_data:\n",
    "                continue\n",
    "            start_time = datetime.now()  # Start time of the batch\n",
    "            print(f\"Batch starting at index {i}, Start time: {start_time}\")\n",
    "            new_batch_text_ids, new_batch_sentence_ids, new_batch_sentences = map(list, zip(*new_batch_data))\n",
    "            \n",
    "            # Schedule multiple Ray tasks for translation\n",
    "            future_results = [predict.remote(pipe_id, sentence) for sentence in new_batch_sentences]\n",
    "            translation_output = ray.get(future_results)\n",
    "            \n",
    "            translated_texts = [out[0][\"translation_text\"] for out in translation_output]\n",
    "            \n",
    "            # Write to JSONL\n",
    "            for text_id, sentence_id, translated_text in zip(new_batch_text_ids, new_batch_sentence_ids, translated_texts):\n",
    "                json.dump({\"text_id\": text_id, \"Sentence_id\": sentence_id, \"translated_sentence\": translated_text}, f)\n",
    "                f.write('\\n')\n",
    "            end_time = datetime.now()  # End time of the batch\n",
    "            duration = end_time - start_time  # Duration of the batch\n",
    "            \n",
    "            print(f\"Batch ending at index {i + batch_size}, End time: {end_time}, Duration: {duration}, part of: {current_language}\")\n",
    "\n",
    "# Shutdown Ray cluster after all tasks\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
