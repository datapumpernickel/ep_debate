{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>id_speaker</th>\n",
       "      <th>Sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41077</td>\n",
       "      <td>3589</td>\n",
       "      <td>178</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>\\n</td>\n",
       "      <td>cat_Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41078</td>\n",
       "      <td>3589</td>\n",
       "      <td>178</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>On Amendment No 90</td>\n",
       "      <td>cat_Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56730</td>\n",
       "      <td>4813</td>\n",
       "      <td>247</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>On Amendment No 40</td>\n",
       "      <td>fra_Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58146</td>\n",
       "      <td>4815</td>\n",
       "      <td>255</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>IN THE CHAIR: MRS HOFF</td>\n",
       "      <td>yue_Hant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>168495</td>\n",
       "      <td>13617</td>\n",
       "      <td>762</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>(Parliament adopted the legislative resolution...</td>\n",
       "      <td>kor_Hang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393919</th>\n",
       "      <td>2073658</td>\n",
       "      <td>179799</td>\n",
       "      <td>14163</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Und da darf ich vielleicht auch erläutern:</td>\n",
       "      <td>deu_Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393920</th>\n",
       "      <td>2073659</td>\n",
       "      <td>179799</td>\n",
       "      <td>14163</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Die Stimmerklärungen ganz am Schluss werden in...</td>\n",
       "      <td>deu_Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393921</th>\n",
       "      <td>2073660</td>\n",
       "      <td>179799</td>\n",
       "      <td>14163</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Und da wir zehn bis 15 Stimmerklärungen haben ...</td>\n",
       "      <td>deu_Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393922</th>\n",
       "      <td>2073661</td>\n",
       "      <td>179799</td>\n",
       "      <td>14163</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Das ist aber auch kein Skandal, sondern alles ...</td>\n",
       "      <td>deu_Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393925</th>\n",
       "      <td>2073664</td>\n",
       "      <td>179801</td>\n",
       "      <td>14165</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Die nächste Tagung findet vom 8. bis zum 9. No...</td>\n",
       "      <td>deu_Latn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>359905 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          index  text_id  session_id  id_speaker  Sentence_id  \\\n",
       "0         41077     3589         178           1            1   \n",
       "1         41078     3589         178           1            2   \n",
       "3         56730     4813         247           1            2   \n",
       "4         58146     4815         255           1            1   \n",
       "6        168495    13617         762           1            2   \n",
       "...         ...      ...         ...         ...          ...   \n",
       "393919  2073658   179799       14163           1            2   \n",
       "393920  2073659   179799       14163           1            3   \n",
       "393921  2073660   179799       14163           1            4   \n",
       "393922  2073661   179799       14163           1            5   \n",
       "393925  2073664   179801       14165           1            1   \n",
       "\n",
       "                                                 sentence  language  \n",
       "0                                                      \\n  cat_Latn  \n",
       "1                                      On Amendment No 90  cat_Latn  \n",
       "3                                      On Amendment No 40  fra_Latn  \n",
       "4                                  IN THE CHAIR: MRS HOFF  yue_Hant  \n",
       "6       (Parliament adopted the legislative resolution...  kor_Hang  \n",
       "...                                                   ...       ...  \n",
       "393919         Und da darf ich vielleicht auch erläutern:  deu_Latn  \n",
       "393920  Die Stimmerklärungen ganz am Schluss werden in...  deu_Latn  \n",
       "393921  Und da wir zehn bis 15 Stimmerklärungen haben ...  deu_Latn  \n",
       "393922  Das ist aber auch kein Skandal, sondern alles ...  deu_Latn  \n",
       "393925  Die nächste Tagung findet vom 8. bis zum 9. No...  deu_Latn  \n",
       "\n",
       "[359905 rows x 7 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"translate_ep_speeches.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1_IH0JaUFfoec2PQqA12_J1x9P58YXWF3\n",
    "\"\"\"\n",
    "\n",
    "#!pip install -U pip transformers\n",
    "#!pip install sentencepiece\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "full_text = pd.read_csv(\"../04_clean_data/language_detection.csv\")\n",
    "data = pd.read_csv(\"../04_clean_data/missing_speeches_parlee_sents.csv\")\n",
    "\n",
    "\n",
    "data.dropna(subset=['sentence'], inplace=True)\n",
    "data.reset_index(inplace = True)\n",
    "\n",
    "filtered_data = pd.merge(data, full_text, on='text_id', how='left')\n",
    "\n",
    "filtered_data = filtered_data[filtered_data['language'] != 'eng_Latn']\n",
    "\n",
    "# Keep only specific columns\n",
    "filtered_data = filtered_data[['text_id', 'session_id', 'id_speaker', 'Sentence_id', 'sentence','language']]\n",
    "\n",
    "filtered_data.reset_index(inplace = True)\n",
    "filtered_data.drop_duplicates(subset='sentence', inplace = True)\n",
    "filtered_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data['language'].unique()\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "checkpoint = 'facebook/nllb-200-distilled-600M'\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 09:07:19,026\tINFO worker.py:1642 -- Started a local Ray instance.\n",
      "Translating by Language:   0%|          | 0/37 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating by Language:   3%|▎         | 1/37 [00:15<09:20, 15.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fra_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating by Language:   5%|▌         | 2/37 [03:13<1:04:48, 111.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yue_Hant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating by Language:   8%|▊         | 3/37 [03:29<38:13, 67.45s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kor_Hang\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating by Language:  11%|█         | 4/37 [03:45<26:00, 47.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glg_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating by Language:  14%|█▎        | 5/37 [04:01<19:07, 35.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spa_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating by Language:  16%|█▌        | 6/37 [07:16<46:30, 90.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deu_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating by Language:  19%|█▉        | 7/37 [14:56<1:45:35, 211.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arb_Arab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating by Language:  22%|██▏       | 8/37 [15:11<1:11:53, 148.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bod_Tibt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating by Language:  24%|██▍       | 9/37 [15:26<49:53, 106.90s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "krc_Cyrl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating by Language:  27%|██▋       | 10/37 [15:42<35:26, 78.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zho_Hans\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating by Language:  30%|██▉       | 11/37 [15:57<25:40, 59.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ita_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch starting at index 9000, Start time: 2023-11-06 09:24:53.792703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ending at index 10000, End time: 2023-11-06 09:46:05.509506, Duration: 0:21:11.716803, part of: ita_Latn\n",
      "Batch starting at index 10000, Start time: 2023-11-06 09:46:15.563630\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import ray\n",
    "from transformers import pipeline\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Ray cluster once, outside of loops\n",
    "ray.init(num_cpus=20, ignore_reinit_error=True)\n",
    "\n",
    "# @ray.remote decorator enables to use this \n",
    "# function in distributed setting\n",
    "@ray.remote\n",
    "def predict(pipeline, text_data):\n",
    "    return pipeline(text_data)\n",
    "\n",
    "# Open the JSONL file to store the translations\n",
    "with open('translated_sentences.jsonl', 'a+') as f:\n",
    "    # Loop through each unique language in the 'language' column with a progress bar\n",
    "    for current_language in tqdm(filtered_data['language'].unique(), desc=\"Translating by Language\"):\n",
    "        print(current_language)\n",
    "        # Filter the DataFrame to only include rows with the current language\n",
    "        language_specific_data = filtered_data[filtered_data['language'] == current_language]\n",
    "\n",
    "        # Initialize the translation pipeline once per language\n",
    "        translation_pipeline = pipeline('translation',\n",
    "                                        model=model,\n",
    "                                        tokenizer=tokenizer,\n",
    "                                        src_lang=current_language,\n",
    "                                        tgt_lang='eng_Latn',\n",
    "                                        max_length=500)\n",
    "        pipe_id = ray.put(translation_pipeline)\n",
    "\n",
    "        batch_size = 1000\n",
    "        # Process sentences in batches\n",
    "        for i in tqdm(range(0, len(language_specific_data), batch_size), desc=f\"Translating Sentences in {current_language}\", leave=False):\n",
    "\n",
    "            batch_data = language_specific_data.iloc[i:i+batch_size]\n",
    "            batch_sentences = batch_data['sentence'].tolist()\n",
    "\n",
    "            # Check existing IDs and filter new batch data\n",
    "            f.seek(0)\n",
    "            existing_ids = [(json.loads(line).get('text_id'), json.loads(line).get('Sentence_id')) for line in f]\n",
    "            new_batch_data = [(row['text_id'], row['Sentence_id'], row['sentence']) for index, row in batch_data.iterrows() if (row['text_id'], row['Sentence_id']) not in existing_ids]\n",
    "            \n",
    "            if not new_batch_data:\n",
    "                continue\n",
    "            start_time = datetime.now()  # Start time of the batch\n",
    "            print(f\"Batch starting at index {i}, Start time: {start_time}\")\n",
    "            new_batch_text_ids, new_batch_sentence_ids, new_batch_sentences = map(list, zip(*new_batch_data))\n",
    "            \n",
    "            # Schedule multiple Ray tasks for translation\n",
    "            future_results = [predict.remote(pipe_id, sentence) for sentence in new_batch_sentences]\n",
    "            translation_output = ray.get(future_results)\n",
    "            \n",
    "            translated_texts = [out[0][\"translation_text\"] for out in translation_output]\n",
    "            \n",
    "            # Write to JSONL\n",
    "            for text_id, sentence_id, translated_text in zip(new_batch_text_ids, new_batch_sentence_ids, translated_texts):\n",
    "                json.dump({\"text_id\": text_id, \"Sentence_id\": sentence_id, \"translated_sentence\": translated_text}, f)\n",
    "                f.write('\\n')\n",
    "            end_time = datetime.now()  # End time of the batch\n",
    "            duration = end_time - start_time  # Duration of the batch\n",
    "            \n",
    "            print(f\"Batch ending at index {i + batch_size}, End time: {end_time}, Duration: {duration}, part of: {current_language}\")\n",
    "\n",
    "# Shutdown Ray cluster after all tasks\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
